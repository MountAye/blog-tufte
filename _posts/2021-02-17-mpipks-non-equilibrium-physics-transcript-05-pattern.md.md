---
layout:      post
title:      "马普所·非平衡态系统中的集体过程·语音转录 | 05. Pattern"
keywords:   "mpipks-transcript"
excerpt:    "How these systems can transit between different order states."
date:        2021-02-17
categories:  post
milestoneID: 32
---

> https://www.youtube.com/watch?v=14SaROmQKCI&feature=emb_logo
>
> How these systems can transit between different order states.

<br> `00:05` actually the first time i managed to set
<br> `00:07` up
<br> `00:07` everything in time but i started a half
<br> `00:11` half an hour earlier to set up the
<br> `00:13` webcam and everything
<br> `00:15` so we have a professional equipment here
<br> `00:16` so that's really a video conference
<br> `00:19` conferencing system and uh
<br> `00:22` but it has like 100 different cables
<br> `00:26` and a couple of devices only to be
<br> `00:28` connected in the right way
<br> `00:29` to make things work and but today i
<br> `00:33` managed
<br> `00:33` managed to set it up in time
<br> `00:36` okay so then let's start our lecture
<br> `00:40` today
<br> `00:41` and um so
<br> `00:44` before i go on i'd like to give you a
<br> `00:47` reminder
<br> `00:48` of actually uh what we did last time
<br> `00:52` uh because that's connected to what
<br> `00:54` we're doing today
<br> `00:56` let me just share the screen
<br> `01:08` so there we go ah

### slide 1

<br> `01:12` works flawlessly perfect okay great
<br> `01:16` uh so just a little reminder of last uh
<br> `01:18` kind lecture
<br> `01:19` the last time we started thinking about
<br> `01:22` how order
<br> `01:23` emerges a non-equilibrium system
<br> `01:27` and the key insight from last in the
<br> `01:30` last lecture is that it's
<br> `01:32` many situation boils down to a balance
<br> `01:34` between
<br> `01:35` noise that create disorder
<br> `01:39` and the propagation of information about
<br> `01:43` interactions through the system
<br> `01:47` so in equilibrium conditions and the
<br> `01:49` thermal equilibrium this is formalized
<br> `01:52` by the entropy and the energy uh that
<br> `01:55` then give rise to the free energy that
<br> `01:57` we need to minimize
<br> `01:58` and then we minimize the free energy we
<br> `02:01` kind of
<br> `02:01` know what is happening yeah and when we
<br> `02:05` look at this equilibrium system mass
<br> `02:08` times are these pointers
<br> `02:09` also called xy model we saw that
<br> `02:13` actually in all dimensions smaller or
<br> `02:16` equal to two
<br> `02:18` fluctuations if we prepare the system in
<br> `02:21` a homogeneous
<br> `02:22` state where all pointers point of all
<br> `02:25` arrows point in the same direction
<br> `02:28` if we twist one arrow or if we perturb
<br> `02:31` the system
<br> `02:32` yeah then this destabilizes
<br> `02:36` the order on the large
<br> `02:39` scale yeah so we calculated how such
<br> `02:42` perturbations propagate through the
<br> `02:45` system that we saw that they build up
<br> `02:47` the the longer you go through the system
<br> `02:50` therefore this
<br> `02:51` distance r to infinity that we had last
<br> `02:53` time
<br> `02:54` this has formalized this idea that you
<br> `02:56` cannot build up a long range
<br> `02:58` order in uh equilibrium by the mermain
<br> `03:01` wagner theorem that basically says that
<br> `03:04` even if you had long range order it
<br> `03:06` doesn't cost
<br> `03:07` any energy to have a very slow
<br> `03:10` perturbation
<br> `03:11` of your spins and this very slow
<br> `03:14` perturbation
<br> `03:15` is called the gold stone mode you can
<br> `03:17` always get these gold stone modes
<br> `03:20` in equilibrium conditions if your
<br> `03:21` symmetry so if your
<br> `03:23` spin or your your microscopic degrees of
<br> `03:26` freedom
<br> `03:27` are continuous now then we went on to
<br> `03:31` equilibrium
<br> `03:32` conditions uh non-equilibrium systems
<br> `03:34` and we thought
<br> `03:35` you know so about what is now if these
<br> `03:38` arrows are moving
<br> `03:39` now they're not only pointing in a
<br> `03:41` direction but they're also moving in the
<br> `03:43` same direction
<br> `03:44` then information about
<br> `03:48` the alignment of uh
<br> `03:51` or the direction does not only spread
<br> `03:54` diffusively so very inefficiently
<br> `03:57` equilibrium system but it can spread
<br> `04:01` very quickly through flows convective
<br> `04:04` flows
<br> `04:05` through the entire system and thereby
<br> `04:08` give rise to long-range order
<br> `04:10` even four dimensions
<br> `04:13` smaller or equal to two now so
<br> `04:16` in non-equilibrium systems we have
<br> `04:18` possibilities to spread the information
<br> `04:21` about alignment or to suppress
<br> `04:24` fluctuation to suppress
<br> `04:25` noise that we don't have in equilibrium
<br> `04:28` conditions
<br> `04:29` and therefore we can get ordered states
<br> `04:32` and non-equilibrium systems
<br> `04:33` even if we cannot have them in very
<br> `04:36` similar
<br> `04:36` equilibrium systems

### slide 2

<br> `04:40` so today i want to go
<br> `04:43` one step further further one step
<br> `04:46` further and one step back
<br> `04:47` actually um so before
<br> `04:50` let's let's take a step back before i
<br> `04:52` begin with today's lecture
<br> `04:54` uh just to see where we are in the
<br> `04:56` lecture so we just
<br> `04:57` now got the idea of how order
<br> `05:01` arises in non-equilibrium systems today
<br> `05:04` we'll
<br> `05:04` talk **how these systems can**  
<br> `05:07` **transit between different order states**
<br> `05:10` so we'll be talking about bifurcations
<br> `05:13` and phase transitions
<br> `05:15` and this
<br> `05:18` lecture today in this lecture today we
<br> `05:20` will focus on
<br> `05:21` very large systems where this noise is
<br> `05:24` not
<br> `05:25` this noise term that we have it's psi is
<br> `05:28` not
<br> `05:28` important and in the next lectures
<br> `05:31` two or three or two lectures or so we
<br> `05:34` will
<br> `05:35` take into account what does noise do to
<br> `05:38` order states to do the transitions
<br> `05:40` between
<br> `05:40` faith and for this we will need methods
<br> `05:43` from renormalization group theory
<br> `05:45` that we will introduce here and then
<br> `05:49` we're done with most of the
<br> `05:52` theoretical physics aspects of this
<br> `05:54` lecture after that we'll
<br> `05:55` start taking a different approach and
<br> `05:58` ask how can we actually
<br> `05:59` see order in
<br> `06:03` big data sets somebody gives you a
<br> `06:05` terabyte of data how can you actually
<br> `06:07` identify these degrees of freedom
<br> `06:11` now if you have not only like a spin
<br> `06:13` system uh
<br> `06:14` one degree of freedom for each spin but
<br> `06:17` if you have 20
<br> `06:18` 000 degrees of freedom how can you
<br> `06:19` actually see whether you have some kind
<br> `06:22` of collective
<br> `06:23` state in your system so that will so
<br> `06:26` we'll have two or three
<br> `06:27` lectures on what is actually data
<br> `06:29` science
<br> `06:30` and then at the end of the lecture at
<br> `06:32` the end of january we'll finish up by
<br> `06:34` putting it all together and see how we
<br> `06:36` can
<br> `06:36` switch between theory and data science
<br> `06:40` and back and how to actually generate
<br> `06:42` hypothesis
<br> `06:43` from the data sets that are currently
<br> `06:45` out there
<br> `06:47` okay so for the today's lecture i
<br> `06:50` uh want to now not ask can we have
<br> `06:54` order or can we not have order i want to
<br> `06:57` ask what kind of order do we have
<br> `07:00` and uh to this i
<br> `07:03` uh want to come back to this formalism
<br> `07:07` that allows us to characterize generally
<br> `07:11` larger systems that have spatial degrees
<br> `07:14` of freedom
<br> `07:15` now that are spatial results so we have
<br> `07:17` a field
<br> `07:19` or an order parameter it's called an
<br> `07:20` order parameter field phi
<br> `07:22` of x t now that gives us
<br> `07:26` uh so for each coordinate in space
<br> `07:29` this phi of x and t gives us
<br> `07:33` the value for example of some
<br> `07:35` concentration
<br> `07:37` or for example the number of infected
<br> `07:39` people
<br> `07:40` in dresden also yeah and then we
<br> `07:44` came up with something that is actually
<br> `07:46` standard in the literature
<br> `07:48` but it's just a very inconvenient way of
<br> `07:50` actually writing down
<br> `07:52` partial differential equations is uh
<br> `07:55` that we said okay we can get
<br> `07:58` basically a large class of systems of
<br> `08:00` dynamics
<br> `08:01` if we take some functional f that
<br> `08:04` depends on phi
<br> `08:06` and we take the derivative with respect
<br> `08:09` to y
<br> `08:10` now with this we can create here on the
<br> `08:12` left hand side
<br> `08:13` all kinds of terms that are functions of
<br> `08:17` fine
<br> `08:17` and of derivatives of sine
<br> `08:20` and then we have our noise terms
<br> `08:25` here and if we our old parameter for
<br> `08:28` something like a chemical reaction
<br> `08:30` our order parameter our concentrations
<br> `08:33` are not conserved
<br> `08:34` now so we can change we can actually
<br> `08:36` increase the total concentration of
<br> `08:38` something
<br> `08:39` then we have equations of the first kind
<br> `08:42` called
<br> `08:42` model a this classification scheme
<br> `08:46` and if we are just moving around
<br> `08:48` concentrations
<br> `08:50` which i was actually not
<br> `08:53` without actually not taking any
<br> `08:55` particles for example out of the system
<br> `08:59` then we have these conservative
<br> `09:02` these conservatives so-called
<br> `09:04` conservative systems
<br> `09:06` which are described by equations of this
<br> `09:08` kind and they're called
<br> `09:10` model e yeah and an example of this
<br> `09:14` functional f here is that we write this
<br> `09:17` as an
<br> `09:18` integral of some potential and
<br> `09:21` something that gives the diffusion term
<br> `09:24` once we take the derivative with respect
<br> `09:26` to the function of the derivative
<br> `09:28` with respect to phi now and this
<br> `09:31` potential here is also related then to a
<br> `09:35` local
<br> `09:35` force you know so that's like a formal
<br> `09:38` analogy
<br> `09:39` and of course this the writing of such
<br> `09:42` kind of
<br> `09:42` systems comes actually from equilibrium
<br> `09:45` statistical physics
<br> `09:46` where this f is actually some free
<br> `09:48` energy
<br> `09:50` functions from generalized free energy
<br> `09:52` for example the fight to the floor
<br> `09:53` theory ginsburg land over a theory
<br> `09:57` and then you have an equilibrium system
<br> `09:59` and you want to know
<br> `10:00` how this equilibrium system evolves and
<br> `10:03` then you just take the functional
<br> `10:04` derivatives with respect to your fields
<br> `10:07` and then you know how your equilibrium
<br> `10:09` system your free energy function
<br> `10:12` your free energy that describes your
<br> `10:14` theory gives rise
<br> `10:15` to dynamics of the field now that comes
<br> `10:19` from equilibrium but it's actually a
<br> `10:20` very inconvenient way for us to write
<br> `10:23` down
<br> `10:23` these kind of equations and uh
<br> `10:26` so i just wanted to tell you this
<br> `10:28` classification scheme
<br> `10:30` but in the following slides i write out
<br> `10:31` the equation directly
<br> `10:33` without going via dysfunctional
<br> `10:35` derivatives here
<br> `10:36` and as i said in this lecture we'll
<br> `10:39` first
<br> `10:40` look at transitions between
<br> `10:43` non-equilibrium states uh
<br> `10:46` in systems that where this noise this
<br> `10:49` psi is not important yeah so that's
<br> `10:52` noise for example is not important
<br> `10:54` if the temperature in the equilibrium
<br> `10:56` system the temperature
<br> `10:58` is zero or very small in a typical
<br> `11:01` biological system
<br> `11:03` noise typically is not important if you
<br> `11:06` have a very large
<br> `11:07` number of uh of uh
<br> `11:11` particles that contribute to a certain
<br> `11:13` process
<br> `11:14` yeah so we know first and the first step
<br> `11:17` we asked so
<br> `11:17` how can we go between different
<br> `11:20` non-equilibrium states
<br> `11:22` how can we switch to to say between
<br> `11:25` different kinds of order and
<br> `11:29` in the next step in the next lectures
<br> `11:30` we'll ask okay what does this noise do
<br> `11:33` and of course i wouldn't have a couple
<br> `11:36` of lectures
<br> `11:37` on this noise if it wouldn't do very
<br> `11:40` interesting things
<br> `11:42` but for now we ignore the noise you know
<br> `11:44` and uh what we'll do is also called
<br> `11:46` mean field theory just look at the
<br> `11:49` partial differential equations that are
<br> `11:52` drawn
<br> `11:53` that are driving these concentration
<br> `11:55` fields
<br> `11:56` phi of x t in the limit of a low
<br> `11:59` temperature or very high
<br> `12:01` particle numbers

### slide 3

<br> `12:05` so
<br> `12:09` for the very first part of this lecture
<br> `12:12` i would like to even go to
<br> `12:14` even to an even simpler
<br> `12:17` framework and that is we don't even
<br> `12:20` consider
<br> `12:21` space now we say the system is well
<br> `12:24` mixed
<br> `12:25` yeah and if the system is well mixed
<br> `12:28` then
<br> `12:29` we can uh neglect spatial derivatives
<br> `12:33` you know because the system is in
<br> `12:35` homogeneous state yeah and then
<br> `12:37` so this is considered like a chemical
<br> `12:39` and we're always stirring the
<br> `12:41` these chemical reactions so that every
<br> `12:43` particle
<br> `12:44` every molecule very rapidly travels
<br> `12:48` through the entire system you know so
<br> `12:50` then we basically
<br> `12:52` have a homogeneous system where
<br> `12:54` concentrations do not depend on space
<br> `12:58` and if the concentrations do not depend
<br> `13:00` on space
<br> `13:01` then uh spatial derivatives become zero
<br> `13:05` yeah and these are the kind of systems
<br> `13:07` that we are looking at
<br> `13:09` formally again in this notation of
<br> `13:12` uh functional derivatives uh we can then
<br> `13:15` neglect
<br> `13:16` the spatial derivatives uh in this
<br> `13:18` functional here
<br> `13:20` and what we then get is a
<br> `13:23` differential equation of phi of t that
<br> `13:26` now does not depend on
<br> `13:28` space anymore and that is this time
<br> `13:30` evolution of this quantity
<br> `13:32` is just described by some function f
<br> `13:35` yeah and of course i could have just
<br> `13:37` started with this equation here without
<br> `13:38` the
<br> `13:39` function of derivatives and so on yeah
<br> `13:41` we just of course what we'll
<br> `13:43` be looking at for for the first part of
<br> `13:45` this lecture
<br> `13:46` are non-linear differential equation or
<br> `13:48` nonlinear
<br> `13:50` dynamics and i'll give you a brief
<br> `13:53` overview because it gives you
<br> `13:55` an insight not only on the
<br> `13:57` renormalization part
<br> `13:58` that we will do before christmas but
<br> `14:00` also to the second
<br> `14:02` part of this lecture we will look on how
<br> `14:04` spatial different spatial structures
<br> `14:06` can emerge so
<br> `14:10` we have these not with these non-linear
<br> `14:12` differential equations here so on the
<br> `14:14` left hand side we have the time
<br> `14:16` evolution of the scalar
<br> `14:18` yeah and on the right hand side we have
<br> `14:19` some function it's a nonlinear function
<br> `14:21` that describes
<br> `14:22` this time evolution and uh
<br> `14:26` of course a typical example of a
<br> `14:28` non-linear system is always in biology
<br> `14:30` in biology everything is nonlinear and
<br> `14:36` very simple system you can look at is
<br> `14:39` for example
<br> `14:40` how the gene
<br> `14:44` uh interacts with itself a
<br> `14:48` self-activation
<br> `14:50` of a gene yeah and
<br> `14:54` if you have a gene i mentioned this
<br> `14:57` already now so you have a gene here
<br> `15:02` now that's part of the dna and at the
<br> `15:05` beginning
<br> `15:06` of this gene also the dna is very long
<br> `15:08` we're now looking at a very
<br> `15:10` short part of the total dna
<br> `15:13` that's the gene here at the beginning
<br> `15:17` of this team there's a promoter the
<br> `15:20` promoter
<br> `15:20` turns on or off this g and when this
<br> `15:24` promoter
<br> `15:25` turns on the gene then this gene
<br> `15:28` produces molecules
<br> `15:30` now actually via multiple steps
<br> `15:33` but in the end you have something that's
<br> `15:36` called a protein
<br> `15:40` now of course i did this of course for
<br> `15:43` the ball it just of course it's more
<br> `15:44` complicated than that yeah and this
<br> `15:47` protein what does this protein do
<br> `15:49` no it can degrade
<br> `15:52` but it can also do fancy things so for
<br> `15:55` this protein is produced and it swims
<br> `15:57` around in the cell
<br> `15:59` you know and we have these proteins now
<br> `16:01` here multiple copies of this because
<br> `16:02` this
<br> `16:03` gene keeps producing proteins and now we
<br> `16:06` can say that this protein
<br> `16:07` also decides whether this gene is on or
<br> `16:10` off
<br> `16:11` and what is a typical situation is that
<br> `16:13` this protein then binds
<br> `16:17` to this promoter to the start site of
<br> `16:20` this
<br> `16:21` and only if we have two of them together
<br> `16:25` we can start the gene
<br> `16:28` now we can start the gene and
<br> `16:31` so that means we need to find pairs
<br> `16:35` between these genes here but between
<br> `16:37` these proteins here
<br> `16:38` and if we have found a pair it can bind
<br> `16:41` and then
<br> `16:42` this starts producing proteins from the
<br> `16:44` gene again
<br> `16:45` which then again couple
<br> `16:49` back to itself so it's a feedback and
<br> `16:53` uh typically the kind of equation that
<br> `16:56` you get from this
<br> `16:57` is from the
<br> `17:01` for the concentration of the numbers
<br> `17:04` of these proteins in the cell
<br> `17:08` is that you have one process that
<br> `17:10` describes
<br> `17:11` the activation of the gene itself
<br> `17:14` and this activation is non-linear so you
<br> `17:17` have many
<br> `17:18` different contributions y squared
<br> `17:22` divided by 1 plus y squared
<br> `17:25` this function here
<br> `17:28` is the activation part
<br> `17:33` it describes that you have to find pairs
<br> `17:36` of these genes now so that you look at
<br> `17:39` this here
<br> `17:40` the more pairs you have the more pairs
<br> `17:42` that's the number of pairs that you can
<br> `17:44` you can build now the more pairs you can
<br> `17:47` build
<br> `17:49` the more likely you express this g here
<br> `17:52` you
<br> `17:52` turn it on but then we also have the
<br> `17:55` situation
<br> `17:56` that if we have too many of these
<br> `17:58` there's a crowding effect on the dna
<br> `18:00` they can't all bind
<br> `18:02` at the same time now so they have to
<br> `18:04` compete for binding
<br> `18:06` so they can't if you have like a million
<br> `18:09` or like infinitely many of these copies
<br> `18:12` here
<br> `18:12` they all cannot bind simultaneously to
<br> `18:15` this region here because this is
<br> `18:17` only a finite amount of space yeah
<br> `18:20` and that's why we have another part here
<br> `18:24` that saturates you know that means that
<br> `18:26` we
<br> `18:29` that we for very high values of this y
<br> `18:32` of this protein concentrations we cannot
<br> `18:34` get any better
<br> `18:36` and this is called the hill function and
<br> `18:38` the hill function typically
<br> `18:40` looks like something like this
<br> `18:49` now it's clearly non-linear and then we
<br> `18:51` have the second term
<br> `18:53` now it describes that describes
<br> `18:56` degradation also and how how many
<br> `18:59` proteins or how many copies of these
<br> `19:01` proteins
<br> `19:02` we use at a given amount of time and
<br> `19:05` this is very simple because the more you
<br> `19:07` have the more you lose now so that's
<br> `19:10` just
<br> `19:10` minus y and this is just as
<br> `19:14` we'll look into detail into this
<br> `19:17` equation
<br> `19:17` later um and this is just an example of
<br> `19:21` how
<br> `19:21` in biological systems these non-linear
<br> `19:24` differential equations automatically
<br> `19:26` emerge
<br> `19:27` almost all the time already on the basic
<br> `19:31` building block of many biological
<br> `19:34` systems namely the expression
<br> `19:36` of a gene so
<br> `19:39` what can we not do with these nonlinear
<br> `19:42` equations
<br> `19:45` and i didn't oh i had i had a transition
<br> `19:48` here i wouldn't have okay so uh
<br> `19:52` so i had a fancy transition i wouldn't
<br> `19:54` have needed to to write that

### slide 4

<br> `19:56` uh okay so let's move on what can we now
<br> `19:58` do
<br> `19:59` with such equations here so the
<br> `20:01` solutions
<br> `20:02` of these non-linear equations that live
<br> `20:05` in some
<br> `20:06` space in some configuration space
<br> `20:10` and in the space we move around you know
<br> `20:14` as the system evolves
<br> `20:16` now uh typically the the space of
<br> `20:20` all possible trajectories of all
<br> `20:22` possible solutions
<br> `20:23` such as the system is called face
<br> `20:25` portrait
<br> `20:26` and that we start
<br> `20:30` at some initial conditions and we
<br> `20:32` involve along
<br> `20:34` this trajectory here if we manage to
<br> `20:36` solve this equation
<br> `20:38` now there's some points that are
<br> `20:40` particularly important
<br> `20:42` in this field namely these are fixed
<br> `20:44` points
<br> `20:46` and these fixed points are points where
<br> `20:49` the time derivative
<br> `20:50` is zero so once you are in these fixed
<br> `20:53` points
<br> `20:54` you cannot get out of that out again
<br> `20:56` because the time derivative
<br> `20:59` is zero now you stay there these are
<br> `21:02` called fixed points
<br> `21:03` and once we know the fixed points we
<br> `21:06` know already
<br> `21:08` a lot about the dynamics of a nonlinear
<br> `21:11` system
<br> `21:13` so now here i have the transition so on
<br> `21:15` the bottom
<br> `21:16` you can see how you can understand the
<br> `21:18` dynamics of nonlinear system
<br> `21:21` just by graphical analysis
<br> `21:24` so now on this diagram at the bottom
<br> `21:28` on the y on the x axis is just the
<br> `21:31` concentration
<br> `21:32` why not just any system we're not just
<br> `21:35` looking at any general system i'll use
<br> `21:37` the y's for simplicity
<br> `21:39` for simplicity uh we have on the excess
<br> `21:41` if we have the concentration
<br> `21:42` y and on the
<br> `21:46` y axis we have what is ever is on the
<br> `21:49` right hand side
<br> `21:51` of our differential equation the times
<br> `21:53` the derivative
<br> `21:55` of y and now
<br> `21:58` we can of course plot this function we
<br> `22:00` can ask how does the right
<br> `22:02` hand side of our differential equation
<br> `22:04` depend
<br> `22:05` on the concentration y now and then we
<br> `22:08` get some function for example the ones
<br> `22:10` that i
<br> `22:10` plotted here and this function
<br> `22:15` will cross the zero line
<br> `22:18` now this function will be zero at
<br> `22:20` certain points
<br> `22:22` and wherever this function is zero this
<br> `22:25` is
<br> `22:25` where we have a fixed point now this is
<br> `22:28` where the
<br> `22:29` time derivative is 0
<br> `22:32` and then we can ask are these fixed
<br> `22:34` points stable
<br> `22:35` or are they unstable so once we're in
<br> `22:37` there do we stay there forever
<br> `22:40` or is it enough if i give a little kick
<br> `22:42` to get out again
<br> `22:44` so how stable are these fixed points
<br> `22:46` once we're in there
<br> `22:47` and that's also something you can very
<br> `22:50` easily
<br> `22:52` see for example if you look at this next
<br> `22:53` point here
<br> `22:55` uh this fixed point here that y
<br> `22:58` dot or the time derivative is zero but
<br> `23:01` if you go a little bit to the right
<br> `23:03` then the time derivative becomes
<br> `23:05` negative
<br> `23:07` yeah so we go back into this point if we
<br> `23:10` go a little bit to the left
<br> `23:12` that the time derivative becomes
<br> `23:13` positive and we also get pushed back
<br> `23:17` into this point so
<br> `23:20` this point here is stable so that means
<br> `23:23` if we go to the right we get pushed back
<br> `23:25` in and if we go to the left we also get
<br> `23:28` pushed back
<br> `23:29` by the time derivative yeah and this is
<br> `23:33` just because
<br> `23:34` um the slope
<br> `23:38` here is negative so the
<br> `23:41` slope of this function of this time
<br> `23:44` derivative
<br> `23:45` now which is of course the same as
<br> `23:48` this here of this function the slope of
<br> `23:52` this function at the fixed point
<br> `23:54` tells us something about the stability
<br> `23:57` here is a so this is a stable fixed
<br> `23:59` point
<br> `23:59` now we always go back here this is an
<br> `24:02` unstable fixed point
<br> `24:03` so we go to the right and then the time
<br> `24:06` derivative
<br> `24:07` is positive so we get even further to
<br> `24:09` the right
<br> `24:10` we go to the left time derivative is
<br> `24:13` negative
<br> `24:14` and we get even further to the left
<br> `24:17` yeah so these fixed points are very
<br> `24:19` important and the stability of these
<br> `24:21` fixed points
<br> `24:22` tells us where our system will evolve
<br> `24:26` so just graphically you can see that if
<br> `24:28` i start here
<br> `24:30` with my system yeah you can you can just
<br> `24:33` graphically see
<br> `24:34` that my the dynamics will go into the
<br> `24:37` stable fixed point
<br> `24:38` and stay there now there are situations
<br> `24:42` so i said these fixed points are very
<br> `24:44` important that characterize
<br> `24:46` their stability the stability of these
<br> `24:49` fixed points characterizes
<br> `24:51` where our system will go carry towards
<br> `24:54` the dynamics of the system
<br> `24:56` and now what happens if we change
<br> `25:00` parameters now if these if we change
<br> `25:05` uh parameters then the number
<br> `25:08` or the kind of fixed point the stability
<br> `25:11` of this fixed point
<br> `25:12` can change and this is if this happens
<br> `25:16` uh that the number of this fixed point
<br> `25:18` or the stability
<br> `25:19` changes then are we talking about a
<br> `25:22` bifurcation
<br> `25:23` called a bifurcation and uh what is this
<br> `25:28` parameter
<br> `25:29` now so we call it r from now on so
<br> `25:31` there's some parameter
<br> `25:33` that makes the number of fixed point
<br> `25:37` uh also this ability change that's what
<br> `25:39` we call a control parameter and
<br> `25:42` typically
<br> `25:43` it's related in many physical systems
<br> `25:45` related to
<br> `25:46` how far you're actually out of thermal
<br> `25:48` equilibrium
<br> `25:50` so this r could be for example be
<br> `25:53` the val the difference in temperatures
<br> `25:56` between two boundaries
<br> `25:57` of the system
<br> `26:01` so these are bifurcations and
<br> `26:05` these bifurcations can be classified i
<br> `26:08` will now have a look at a few examples
<br> `26:10` of these
<br> `26:10` bifurcations so uh

### slide 5

<br> `26:15` the simplest bifurcations or one of the
<br> `26:18` simplest bifurcations you can get
<br> `26:20` is if you consider nonlinear equations
<br> `26:23` of this kind here now so this is
<br> `26:26` for each bifurcation i show you the
<br> `26:29` simplest
<br> `26:31` differential equation that gives rise to
<br> `26:34` such a bifurcation
<br> `26:35` and the simplest equation is also often
<br> `26:38` called a normal form
<br> `26:41` so let's have a look at this equation
<br> `26:45` now suppose that r is smaller than zero
<br> `26:48` so this equation here
<br> `26:49` so we plot the same thing as as on the
<br> `26:51` previous slide so on the
<br> `26:53` right hand on the y-axis we have the
<br> `26:56` time derivative
<br> `26:57` now which is just equal to whatever is
<br> `26:59` on the right-hand side
<br> `27:01` and on the x-axis we have our
<br> `27:03` concentrations
<br> `27:05` now if this r is negative now then we
<br> `27:08` just have a simple
<br> `27:09` parabola that is shifted that is shifted
<br> `27:12` down
<br> `27:13` now and if you have that we can do the
<br> `27:15` same argument as
<br> `27:16` previously so we are at this fixed point
<br> `27:19` we go to the right
<br> `27:20` and then the time derivative gets
<br> `27:22` negative so you would really push back
<br> `27:25` now into this fixed point so this fixed
<br> `27:27` point is stable
<br> `27:29` and then on the right hand side we have
<br> `27:30` another fixed point which is
<br> `27:32` unstable in the middle
<br> `27:36` if r is exactly equal to zero
<br> `27:39` then we have a parabola well it's not
<br> `27:42` hard to see on this parabola we have a
<br> `27:45` weird fixed point here at the bottom
<br> `27:47` now we're not really sure whether it's
<br> `27:49` unstable or not
<br> `27:50` it's at the just at the boundary between
<br> `27:53` stable and unstable
<br> `27:55` so we go here it's stable from the right
<br> `27:57` hand side
<br> `27:58` and unstable from the left hand side the
<br> `28:01` typical notation is
<br> `28:02` that filled circles of this
<br> `28:06` are denotes stable fixed points and
<br> `28:08` these open
<br> `28:10` or white circles denote
<br> `28:13` unstable fixed points and then here the
<br> `28:15` idea is that this
<br> `28:16` fixed point is stable from the left and
<br> `28:18` unstable
<br> `28:19` to the right now and then we set r to
<br> `28:22` positive values
<br> `28:23` then we don't have any fixed point at
<br> `28:25` all and our system
<br> `28:27` will just go to infinity now the time
<br> `28:30` derivative
<br> `28:31` is always positive it will just go to
<br> `28:33` infinity and there are no fixed points
<br> `28:36` what we can now do is we can plot
<br> `28:40` the location of these fixed points that
<br> `28:44` i've given you now for three specific
<br> `28:46` values
<br> `28:47` of this r this parameter r
<br> `28:50` we can plot the location of these fixed
<br> `28:52` points
<br> `28:53` continuously as a function of r and
<br> `28:56` that's
<br> `28:57` depicted in the so-called bifurcation
<br> `28:59` diagram
<br> `29:00` now and i'm showing you here the
<br> `29:02` bifurcation diagram
<br> `29:04` of non-linear differential equation that
<br> `29:06` you see on top here
<br> `29:08` and in this case you can see
<br> `29:12` that the fixed points location of fixed
<br> `29:14` points were here
<br> `29:15` we have a stable fixed point negative
<br> `29:18` values
<br> `29:19` and then an unstable fixed point at
<br> `29:21` positive values
<br> `29:23` and then as we go increase our values of
<br> `29:26` r these two fixed points merge
<br> `29:29` and we end up on the right hand side
<br> `29:31` with a state
<br> `29:32` where we don't have any fixed point at
<br> `29:34` all and we just go
<br> `29:36` to a very high values of the
<br> `29:39` concentration
<br> `29:40` of y and so this is how to read these
<br> `29:43` bifurcation diagrams

### slide 6

<br> `29:45` and just to give you an example just to
<br> `29:49` come back
<br> `29:50` to the self-activation of this gene now
<br> `29:53` this example
<br> `29:54` here we have the nonlinear differential
<br> `29:57` equation again
<br> `29:58` protein concentrations
<br> `30:01` activation term this non-linear term
<br> `30:05` that's actually something you can
<br> `30:06` calculate in a longer calculation
<br> `30:08` but it's already clear from the from
<br> `30:11` these pictures that there's something
<br> `30:12` non-linear
<br> `30:13` coming up and the degradation term
<br> `30:16` you know and then we plot both terms
<br> `30:19` separately
<br> `30:20` and we get something like this here this
<br> `30:22` is the activation term
<br> `30:24` this is the degradation term and uh
<br> `30:27` if we sum them up we get something
<br> `30:30` that looks like what we have here in the
<br> `30:32` middle
<br> `30:33` you know where we then is basically the
<br> `30:36` same plot i showed you in the last slide
<br> `30:38` we plot the right-hand side as a
<br> `30:40` function of y
<br> `30:41` and then we see that for small values
<br> `30:45` of r here we have just one fixed point
<br> `30:50` and as we increase r this
<br> `30:53` non-linear term will become more and
<br> `30:55` more important
<br> `30:57` now and we start
<br> `31:00` having intersections with the x-axis so
<br> `31:04` with zero
<br> `31:06` and if we have a very large and strong
<br> `31:08` activation
<br> `31:09` very strong feedback on itself we have
<br> `31:12` multiple fixed points here
<br> `31:15` the right-hand side is the bifurcation
<br> `31:17` diagram
<br> `31:18` this bifurcation diagram so here's the
<br> `31:22` fixed point and uh this bifurcation
<br> `31:25` diagram looks
<br> `31:26` as as follows yeah so as you go have
<br> `31:29` very
<br> `31:29` for very low values now it's just a
<br> `31:33` translation of these red
<br> `31:34` points here uh from from the previous
<br> `31:38` picture
<br> `31:38` for very low values you have a stable
<br> `31:40` fixed point here
<br> `31:42` at zero and then suddenly this
<br> `31:45` non-linearity or this
<br> `31:47` sigmoidal curve kicks in becomes
<br> `31:50` important
<br> `31:51` and you start intersecting with zero
<br> `31:54` yeah and that happens here at this
<br> `31:57` what's called a bifurcation point
<br> `31:59` and then this bifurcate and if you go
<br> `32:01` past this bifurcation point
<br> `32:03` you go to a state you have a stable
<br> `32:05` state here
<br> `32:07` and an unstable state here and a stable
<br> `32:10` state at the bottom
<br> `32:12` just remains there all the time at zero
<br> `32:17` so this is an example of a saddle node
<br> `32:19` bifurcation so it has this typical
<br> `32:21` signature here of the saddle node
<br> `32:23` bifurcation a little bit more
<br> `32:24` complicated
<br> `32:25` yeah but what you see here is what
<br> `32:28` happens
<br> `32:29` if you turn on the self activation if
<br> `32:32` you turn on the non-linear term
<br> `32:35` you go to a regime you go through this
<br> `32:37` bifurcation
<br> `32:38` here where your system has two stable
<br> `32:42` fixed points you know two stable fixed
<br> `32:45` points
<br> `32:46` are here and here and if you have two
<br> `32:50` stable fixed points that are separated
<br> `32:52` by an eight unstable one then you have a
<br> `32:54` switch
<br> `32:55` so this is just an example of how
<br> `32:59` biological systems in this type of gene
<br> `33:02` can make use of this nor these
<br> `33:04` non-linear effects
<br> `33:06` that they get for example here by having
<br> `33:09` clusters or little pairs you're
<br> `33:11` requiring to have pairs
<br> `33:12` of proteins to bind to the starting
<br> `33:14` region of the gene
<br> `33:15` how they can make use of this nonlinear
<br> `33:17` in fact
<br> `33:19` to in this case build a switch
<br> `33:22` and with this switch if you have a
<br> `33:24` switch you can it's like a bit
<br> `33:26` you can actually store memory in a
<br> `33:29` stable way
<br> `33:30` and uh this is one of the simplest ways
<br> `33:33` that
<br> `33:34` uh biological systems or that cell can
<br> `33:36` store
<br> `33:37` information okay so let's go on i'll

### slide 7

<br> `33:40` just show you some other
<br> `33:42` bifurcations now so we have here
<br> `33:45` certainly different differential
<br> `33:46` equation now with
<br> `33:48` r times y plus y to the
<br> `33:52` power of 3 and then if you just look at
<br> `33:55` the right hand side and you just do this
<br> `33:57` graphical analysis
<br> `33:58` you will see that for small values of r
<br> `34:00` you have a single fixed point
<br> `34:02` single intersection and then if you look
<br> `34:05` at the slope
<br> `34:07` now you can see that this is actually
<br> `34:09` stable as you go to the right
<br> `34:11` and the derivative becomes negative so
<br> `34:13` you get
<br> `34:14` pushed back you go to the left and the
<br> `34:16` derivative becomes positive it always
<br> `34:18` pushes you in the opposite direction
<br> `34:20` so one stable fixed point so
<br> `34:23` if r is exactly equal to zero you're
<br> `34:26` still stable but you're in this real
<br> `34:28` state
<br> `34:29` where you have um a flat
<br> `34:33` as your function goes to tangential
<br> `34:37` to the zero axis to the y-axis
<br> `34:40` and um that oh sorry
<br> `34:45` i activate it and that reminds us a
<br> `34:48` little bit
<br> `34:49` to second order phase transitions you
<br> `34:52` know so that you get
<br> `34:53` tangentials because you're in a state
<br> `34:54` where you whether you don't really know
<br> `34:56` where they should go left or right
<br> `34:59` now so that this function is flat you
<br> `35:01` can go
<br> `35:02` you can go left and right but uh it's
<br> `35:05` not really punished
<br> `35:06` so you can you can you can go left here
<br> `35:09` but because this function is rather flat
<br> `35:11` now you can
<br> `35:12` stay there for a long time you go right
<br> `35:14` and the function is very flat
<br> `35:16` now it's tangential you can also stay
<br> `35:18` there
<br> `35:19` and that's probably what you know from
<br> `35:21` the potential
<br> `35:22` in the isaac mode for example second
<br> `35:25` order phase transitions
<br> `35:26` where also at the critical point the
<br> `35:29` potential becomes flat
<br> `35:30` and then fluctuations to the left and to
<br> `35:33` the right and spins
<br> `35:35` are not punished anymore and you get
<br> `35:36` these long range
<br> `35:38` correlations in in the fluctuations and
<br> `35:41` all these weird effects of criticality
<br> `35:44` now if you increase r further
<br> `35:48` then you get something like this here
<br> `35:50` you've got two fixed points
<br> `35:55` you get two fixed points two stable ones
<br> `35:57` and an unstable one
<br> `35:58` in the middle and the bifurcation
<br> `36:00` diagram looks like this here
<br> `36:02` uh so for low values of r you have
<br> `36:05` one stable fixed point at zero
<br> `36:09` and then as r increases beyond
<br> `36:12` uh critical value beyond r equals zero
<br> `36:16` you get this branching into two stable
<br> `36:19` states
<br> `36:20` that are separated by an unstable state
<br> `36:23` and if you now compare again to the
<br> `36:25` ising model
<br> `36:27` uh this is exactly how the magnetization
<br> `36:29` looks like
<br> `36:30` as a function of the temperature of the
<br> `36:33` inverse temperature
<br> `36:34` this looks like an icing mold where you
<br> `36:36` lower the temperature
<br> `36:42` so this was a so-called super critical
<br> `36:44` pitch for
<br> `36:45` bifurcation and uh there's a super
<br> `36:48` critical pitch book bifurcation that's
<br> `36:50` also a subcritical
<br> `36:52` pitchfork bifurcation now that looks um

### slide 8

<br> `36:55` like this here and there is an error in
<br> `36:58` this
<br> `36:59` formula let me just check
<br> `37:06` um
<br> `37:08` here sorry
<br> `37:11` there's a minus sign
<br> `37:15` it should be minus here
<br> `37:19` now for this to make sense for these uh
<br> `37:21` cross made sense needs to be a minus
<br> `37:23` and now we have the same thing but with
<br> `37:25` a plus
<br> `37:27` yeah and if we have this plus then we
<br> `37:29` just turn around
<br> `37:30` the diagrams that we have the previous
<br> `37:33` slide
<br> `37:34` so for low values for negative values of
<br> `37:38` control parameter r uh we get three
<br> `37:40` fixed points
<br> `37:42` one stable fixed point in the middle and
<br> `37:44` two unstable fixed point
<br> `37:46` points at boundaries as we increase
<br> `37:49` r we have one unstable fixed point
<br> `37:53` at uh y equals zero and
<br> `37:56` uh this fixed point stays unstable
<br> `38:00` as we increase the value of r
<br> `38:04` so here's the bifurcation diagram
<br> `38:07` so we start with low values of r where
<br> `38:09` you have a stable
<br> `38:11` fixed point at the concentration zero
<br> `38:15` and two unstable fixed points
<br> `38:19` around that so because they're unstable
<br> `38:22` you have to go this way
<br> `38:24` as well and then as you increase
<br> `38:28` this value of r you go to a state
<br> `38:31` where your fixed points the stable fixed
<br> `38:34` point you've been in
<br> `38:35` suddenly becomes unstable yeah and
<br> `38:40` what you have here is now that if you go
<br> `38:43` here so here you stay at zero
<br> `38:45` you stay at zero all the time and then
<br> `38:48` you go to this state and then you don't
<br> `38:50` go to something small
<br> `38:51` but you immediately to go go to
<br> `38:53` something very large to infinity if
<br> `38:56` there's no other thing that
<br> `38:57` stops you from doing that and that's a
<br> `39:00` sub critical bifurcation
<br> `39:01` where it's because you have this uh this
<br> `39:04` uh
<br> `39:06` this discontinuity in the state of your
<br> `39:08` system so here it was zero
<br> `39:11` and suddenly it becomes very something
<br> `39:12` very large
<br> `39:14` if you compare that to a supercritical
<br> `39:16` bifurcation
<br> `39:18` our state was zero for small values of r
<br> `39:21` and then continuously increased so if
<br> `39:24` this was a
<br> `39:25` was resembling a second order phase
<br> `39:27` position
<br> `39:28` now this is resembling a first order
<br> `39:30` phase transition
<br> `39:31` right in the isaac model for example if
<br> `39:34` you change the
<br> `39:35` magnetic fields at low temperatures
<br> `39:39` so so what's so i'm making this
<br> `39:42` correspondence to
<br> `39:44` ising models and uh equilibrium systems
<br> `39:47` and phase transitions here
<br> `39:49` so what's the difference between a
<br> `39:50` bifurcation and a face
<br> `39:52` uh transition so biovocation
<br> `39:56` bifurcations actually resemble phase
<br> `39:58` transitions
<br> `39:59` in specific cases namely when
<br> `40:05` our when this here is actually a free
<br> `40:09` energy yeah and that's
<br> `40:10` that's that's the beautiful thing about
<br> `40:12` this writing uh these nonlinear
<br> `40:14` equations in this
<br> `40:15` specific form yeah so if this here is
<br> `40:18` actually a **free energy**
<br> `40:20` like the vinsmoke lambda or free energy
<br> `40:22` function for example that describes
<br> `40:23` things like the ising model
<br> `40:25` then the bifurcations correspond our
<br> `40:29` generalization of phase transitions
<br> `40:33` now you have many bifurcations
<br> `40:36` mainly possible bifurcations including
<br> `40:40` bifurcations that have
<br> `40:42` imaginary components so that give rise
<br> `40:45` to
<br> `40:45` imaginary components then that means
<br> `40:48` that you have
<br> `40:49` oscillations in time so that's that's
<br> `40:52` also something that
<br> `40:53` that you can have in these bifurcations
<br> `40:55` and but we'll not be dealing with that
<br> `40:57` i'll show you one more kind of
<br> `40:59` bifurcation and that's
<br> `41:01` a trans critical modification i'm
<br> `41:03` showing you that because it's relevant
<br> `41:04` for epidemics
<br> `41:06` and for the things that we'll be doing
<br> `41:07` before christmas
<br> `41:09` yeah and because we're now probably
<br> `41:11` going into lockdown
<br> `41:13` sooner sooner than later in jason and
<br> `41:15` also have
<br> `41:16` spent the last rest of the time for
<br> `41:18` christmas working on
<br> `41:20` epidemic models i'll explain to you
<br> `41:22` renormalization on
<br> `41:23` epidemic models yeah and this is an
<br> `41:26` example of an epidemic model i'll show
<br> `41:28` you in the next slide
<br> `41:29` why this is the case this is just again
<br> `41:31` now the simplest equation that gives you
<br> `41:34` this kind of behavior so r times y minus
<br> `41:37` y squared but now you do the usual
<br> `41:40` graphical analysis
<br> `41:42` and what you see is that you have this
<br> `41:44` inverted
<br> `41:45` parabola and if r is smaller than zero
<br> `41:49` you're gonna have something like this
<br> `41:50` here
<br> `41:51` and if you increase r yeah
<br> `41:54` then you move uh to a single fixed point
<br> `41:58` and then you go to a stable fixed point
<br> `42:01` at positive values of r
<br> `42:04` so and this is the bifurcation diagram
<br> `42:06` here at the bottom
<br> `42:08` i'll show you how to in the next slide
<br> `42:10` i'll give you an example
<br> `42:11` we have a stable branch for low values
<br> `42:14` of r
<br> `42:15` and then you go and an unstable branch
<br> `42:18` here for negative values of
<br> `42:19` y because stable at zero at negative
<br> `42:22` and unstable negative values and then
<br> `42:25` you flip
<br> `42:25` things around and the unstable branch
<br> `42:29` the zero point becomes unstable and
<br> `42:33` this diagonal line here this linear
<br> `42:36` state
<br> `42:36` becomes stable that's called a
<br> `42:38` trans-critical bifurcation and you just
<br> `42:41` flip things around basically
<br> `42:44` now let's have a little look at such an
<br> `42:47` example of a trans-critical
<br> `42:49` bifurcation now so suppose

### slide 10

<br> `42:53` you have a disease now let's not give it
<br> `42:56` a name
<br> `42:57` so last year last year i gave a lecture
<br> `43:00` and i introduced disease models a few
<br> `43:03` series of disease models
<br> `43:05` and it was february last year and
<br> `43:08` these disease models at this point i
<br> `43:11` called it the rouhan
<br> `43:13` virus because at this point of the wuhan
<br> `43:16` model because at this point
<br> `43:17` the pandemic was restricted to this one
<br> `43:19` city in china
<br> `43:21` but now it's a little bit more general
<br> `43:24` and that's now we call it i don't know
<br> `43:26` the world iris or whatever now so this
<br> `43:30` model looks very similar simple
<br> `43:32` now so you have two kinds of people and
<br> `43:34` also the the infected ones
<br> `43:36` and the susceptible ones not infected
<br> `43:39` ones
<br> `43:40` they carry the disease they carry the
<br> `43:41` virus and the susceptible ones
<br> `43:44` they are healthy but they can catch the
<br> `43:48` virus
<br> `43:50` it's the simplest disease model you can
<br> `43:52` think about it's called also called the
<br> `43:53` because you have these two uh two
<br> `43:56` letters called the s
<br> `43:57` i model or contact process now
<br> `44:01` we can write down some simple chemical
<br> `44:04` reactions
<br> `44:05` some pseudo chemical so if an
<br> `44:08` infected person meets a susceptible
<br> `44:10` person or a healthy person
<br> `44:12` then with a rate lambda the the
<br> `44:15` susceptible person
<br> `44:16` turns into another infected person
<br> `44:20` and we have two infected persons at the
<br> `44:23` end of this
<br> `44:24` reaction yeah and then
<br> `44:28` the second thing that can happen is that
<br> `44:29` an affected person at some point
<br> `44:31` recovers
<br> `44:32` you know and if you recover you turn an
<br> `44:35` infected person
<br> `44:37` back to a susceptible one now that's the
<br> `44:40` simplest thing you can imagine in terms
<br> `44:42` of disease spreading
<br> `44:43` and now we can have a simple look at uh
<br> `44:46` how we
<br> `44:47` understand the non-linear dynamics of
<br> `44:50` the system
<br> `44:54` so first we just write down differential
<br> `44:57` equations
<br> `44:58` what is the time derivative
<br> `45:02` of the concentration of these i people
<br> `45:06` now we can write down this time
<br> `45:07` derivative so this the number of
<br> `45:09` infected people
<br> `45:11` increases with the rate lambda
<br> `45:14` and this rate of increase is a
<br> `45:17` proportional to the probability that is
<br> `45:19` a susceptible person meets
<br> `45:22` gets in touch with an infected person
<br> `45:25` and the more affected and the more
<br> `45:27` susceptible people we have
<br> `45:29` the higher is the spreading rate so this
<br> `45:32` is
<br> `45:32` proportional to s times i
<br> `45:36` and then an infected person can
<br> `45:39` turn back into a susceptible one
<br> `45:43` so that means we have minus s i
<br> `45:47` minus mu i
<br> `45:50` we can write down a similar equation for
<br> `45:52` the susceptible people
<br> `45:54` d over dts and that's just the reverse
<br> `45:58` now so the the negative of this so
<br> `46:01` we lose so it's a infected people by
<br> `46:05` infections number times as i times
<br> `46:08` s times i and plus
<br> `46:12` whenever an infected people uh
<br> `46:15` recovers
<br> `46:19` we get another susceptible one
<br> `46:22` and uh what we also say is that's uh
<br> `46:25` such a simplification this is an
<br> `46:28` important simplification
<br> `46:29` is that the total number of people
<br> `46:33` stays constant like we say this is a
<br> `46:37` total concentration of both gas content
<br> `46:39` so this hectic turns into susceptible a
<br> `46:42` susceptible tends to affect it
<br> `46:43` but actually people don't die from the
<br> `46:46` disease
<br> `46:47` so the number of people that we have
<br> `46:50` remains constant
<br> `46:52` now we can plug this condition in
<br> `46:56` then we get d over dti
<br> `46:59` is equal to lambda i i minus 1
<br> `47:03` now we just plug this in minus
<br> `47:08` ui and then we can get the fixed points
<br> `47:12` by just setting this to zero
<br> `47:15` the fixed points are given by i
<br> `47:18` times lambda 1 minus i
<br> `47:23` minus mu is equal
<br> `47:27` to zero so this is not the imaginary i
<br> `47:29` of course that is just the infected
<br> `47:32` and uh so this is the condition if we
<br> `47:34` set the left-hand side of these
<br> `47:35` equations to zero
<br> `47:37` and then we get a condition for the
<br> `47:38` fixed point and then we can solve this
<br> `47:41` and say okay i one is zero
<br> `47:45` the first pixel fixed point is at zero
<br> `47:47` so we can solve this equation by setting
<br> `47:49` i to zero
<br> `47:50` we can solve this equation also by
<br> `47:53` setting
<br> `47:54` i to lambda minus
<br> `47:57` u over lambda now that's another
<br> `48:00` solution
<br> `48:02` which is equal to one minus mu over
<br> `48:06` lambda this tells us already that this
<br> `48:09` mu over lambda is something important
<br> `48:11` the ratio between the time scales the
<br> `48:13` rates of these processes
<br> `48:15` is something important because it pops
<br> `48:17` up here
<br> `48:18` in the fixed points as a ratio
<br> `48:22` and now now we say if you evaluate now
<br> `48:25` the right hand side
<br> `48:26` of this equation at the fixed point to
<br> `48:28` get the stability
<br> `48:30` yeah so the time derivative with of
<br> `48:33` this right-hand side that's called f
<br> `48:36` and that is just given by lambda
<br> `48:40` 1 minus 2i minus
<br> `48:43` mu and
<br> `48:46` now we evaluate this time derivative
<br> `48:49` this derivative
<br> `48:50` at the fixed point so the first fixed
<br> `48:53` point
<br> `48:54` is zero
<br> `48:57` and at this fixed point we have lambda
<br> `49:01` minus mu where we plug that in and the
<br> `49:03` second fixed point is
<br> `49:05` f prime of 1 minus nu over lambda
<br> `49:09` and then we have that this is
<br> `49:13` u minus number
<br> `49:16` so this looks a little bit symmetrical
<br> `49:18` right and this reminds of
<br> `49:20` us of the this transcritical bifurcation
<br> `49:23` that we had
<br> `49:24` and uh if we plot things then we see
<br> `49:28` that that's actually what's happening
<br> `49:30` now we plot the fixed points
<br> `49:31` now this is ice sorry
<br> `49:35` i star as a function
<br> `49:39` of mu over lambda
<br> `49:42` all right so then we have the staple
<br> `49:44` fixed point
<br> `49:46` so we see here that there's that the
<br> `49:48` signs of this fixed point
<br> `49:51` whether they're stable or not that
<br> `49:53` depends
<br> `49:54` on whether this what is the whether mu
<br> `49:58` is larger than lambda or not
<br> `50:01` so something is happening here at one
<br> `50:04` and now we plot this fixed point so one
<br> `50:06` is at zero
<br> `50:07` and for low values if uh
<br> `50:11` if lambda is larger than mu yeah
<br> `50:14` then this fixed point here is unstable
<br> `50:19` that's the dashed line and the other
<br> `50:22` fixed point
<br> `50:23` just has the opposite stability it's
<br> `50:26` stable
<br> `50:27` goes like this and then
<br> `50:31` if at that lambda here at mu over lambda
<br> `50:35` equals to one we have this change where
<br> `50:38` now
<br> `50:40` this zero fixed point this one here
<br> `50:44` becomes stable and the other fixed point
<br> `50:49` becomes unstable so this simple disease
<br> `50:52` model shows a transcritical
<br> `50:54` bifurcation and if we now take into
<br> `50:58` account fluctuations that we will
<br> `51:00` do that before christmas we'll see that
<br> `51:02` this is actually
<br> `51:05` that this model is actually one of the
<br> `51:08` fundamental model
<br> `51:09` to understand criticality and
<br> `51:12` non-equilibrium systems so this
<br> `51:14` simple model describes the large class
<br> `51:16` of
<br> `51:18` critical behaviors in non-equilibrium
<br> `51:20` system and we'll see that in the
<br> `51:22` following lectures
<br> `51:24` so this was just uh briefly a discussion
<br> `51:27` of what can happen
<br> `51:29` if homogeneous states change if you
<br> `51:32` don't have states
<br> `51:34` and uh so that was something that they
<br> `51:37` basically the foundations of nonlinear
<br> `51:39` dynamics

### slide 11

<br> `51:40` many of you will already have heard of
<br> `51:41` that and
<br> `51:43` of course non-equilibrium systems
<br> `51:46` have this capacity that they're able to
<br> `51:49` produce
<br> `51:50` very complex uh structures so if you
<br> `51:52` think you're just in space i first think
<br> `51:54` for example
<br> `51:54` about biological system think about a
<br> `51:57` cell and all of this stuff that is
<br> `51:58` highly organized in the cell
<br> `52:01` yeah so in this second part of this
<br> `52:04` lecture we will now want to understand
<br> `52:06` if we not only have transitions between
<br> `52:08` homogeneous states
<br> `52:09` but can we also have transitions between
<br> `52:11` homogeneous states
<br> `52:13` yeah so where that have no spatial
<br> `52:15` structure where all for example
<br> `52:17` all arrows or all spins point in the
<br> `52:20` same direction
<br> `52:21` and states where we actually have um
<br> `52:24` a spatial pattern or a spatial structure
<br> `52:28` and a nice example so one of my favorite
<br> `52:31` examples
<br> `52:32` is actually you can see here on the
<br> `52:33` surface of jupiter
<br> `52:35` and you can see
<br> `52:38` now a satellite image of jupiter here
<br> `52:41` and what you see is that you have here
<br> `52:45` these stripes
<br> `52:47` on the surface of jupiter now you have
<br> `52:49` stripes
<br> `52:50` of different color of different kinds
<br> `52:53` and what's actually happening here is
<br> `52:55` that you have
<br> `52:56` a balance between uh convective
<br> `53:00` processes so
<br> `53:02` so gas that is that comes from a jupiter
<br> `53:05` from the
<br> `53:06` core of jupiter and that rises to the
<br> `53:07` surface and then goes back
<br> `53:10` and you have shear flow also where these
<br> `53:13` actually if you look at jupiter as a
<br> `53:14` movie
<br> `53:15` after that you will see that some of
<br> `53:18` these drives travel in the left
<br> `53:19` direction
<br> `53:20` and others travel in the right direction
<br> `53:22` it's very it's very very cool actually
<br> `53:24` and the reason for this is that it is a
<br> `53:27` non-equilibrium system
<br> `53:29` and once that one that fits very well
<br> `53:32` into our definition that we had in the
<br> `53:34` first lecture
<br> `53:35` namely the system is coupled to
<br> `53:37` different bars
<br> `53:38` and so this jupiter is hot inside
<br> `53:43` and cold outside so on the outside and
<br> `53:46` we have space
<br> `53:47` space and that's very cold and inside
<br> `53:49` jupiter is very hot
<br> `53:50` yeah and if you do that you have
<br> `53:52` something hot and something cold
<br> `53:55` now you know that frog maybe from your
<br> `53:56` room then you get conductive flow so the
<br> `53:59` air goes up
<br> `54:00` cools down goes up cools down
<br> `54:04` gets heated up cools down and so on you
<br> `54:07` get these convective flows
<br> `54:08` and that generates these patterns on
<br> `54:12` jupiter
<br> `54:13` and the origin of these patterns of this
<br> `54:15` conductive flow
<br> `54:16` is that you have this incompatible bath
<br> `54:19` the cold bath
<br> `54:20` or the cold boundary or the hot boundary
<br> `54:23` at the bottom
<br> `54:24` and the code boundary at the top and
<br> `54:26` that gives rise to
<br> `54:28` spatial and dynamical structures that
<br> `54:30` look very interesting

### slide 12

<br> `54:35` so now we go back to our little
<br> `54:38` uh a little general functional
<br> `54:41` definition of spatial
<br> `54:43` uh launch voice system again we look uh
<br> `54:46` we ignore the noise again
<br> `54:48` and again also if you if you're not
<br> `54:50` familiar if you're not very happy with
<br> `54:52` these functional
<br> `54:54` derivatives uh i always write down the
<br> `54:56` specific
<br> `54:57` equations that we're actually studying
<br> `54:59` at the following but this was the
<br> `55:00` general framework that we studied and
<br> `55:03` that we introduced
<br> `55:04` that incorporates both the conservative
<br> `55:06` and the non-conservative models the
<br> `55:08` model a
<br> `55:08` and b and we suppose that there's some
<br> `55:11` parameter
<br> `55:13` r here
<br> `55:16` that describes how our system goes out
<br> `55:20` of equilibrium
<br> `55:21` also that typically describes
<br> `55:24` a transition a control parameter that
<br> `55:27` was previously bifurcation
<br> `55:29` but that now describes a state where we
<br> `55:31` go from
<br> `55:32` a spatial homogeneous spatially
<br> `55:34` homogeneous
<br> `55:35` solution spatially homogeneous system to
<br> `55:38` a system
<br> `55:39` that is spatially structured now and
<br> `55:42` that's also what's here on the right
<br> `55:43` hand side
<br> `55:44` yeah and you have this parameter r and
<br> `55:47` if you increase
<br> `55:49` this parameter r and you ask
<br> `55:52` whether or not you have a spatial
<br> `55:55` pattern
<br> `55:56` then you want to understand this
<br> `55:58` transition between
<br> `56:00` the stage where you don't have any
<br> `56:02` pattern no the homogeneous day the
<br> `56:04` boring state
<br> `56:05` and the stage where your system is
<br> `56:07` structured and it has a characteristic
<br> `56:10` wavelength
<br> `56:11` and so on and this parameter we call
<br> `56:14` r again and
<br> `56:18` an example of such a system now as you
<br> `56:21` can see here so if we
<br> `56:22` plug in some values for this function
<br> `56:25` here
<br> `56:25` we get and partial differential
<br> `56:27` equations where we have a time
<br> `56:29` derivative
<br> `56:30` here again on the on the left hand side
<br> `56:33` and we have some non-linear terms
<br> `56:35` here on the left hand side but we also
<br> `56:39` have and this
<br> `56:40` actually looks like something that we've
<br> `56:41` seen i probably was the supercritical
<br> `56:45` bifurcation but we also have
<br> `56:48` spatial derivatives of any order so here
<br> `56:51` we have
<br> `56:52` the second spatial derivative like
<br> `56:54` diffusion
<br> `56:55` term and we have a fourth order
<br> `56:58` spatial derivatives of the fourth
<br> `57:00` spatial derivative
<br> `57:02` with respect to space now so this is an
<br> `57:05` example of the kind of systems
<br> `57:08` that describe spatially extended
<br> `57:11` systems if we neglect noise
<br> `57:18` so how do we now study this kind of
<br> `57:20` systems
<br> `57:21` yeah so how do we study that the idea
<br> `57:24` is that like in many
<br> `57:28` cases in physics that we look very
<br> `57:31` closely
<br> `57:32` at these points here we look very
<br> `57:35` closely at the point
<br> `57:37` when we see a pattern emerge for the
<br> `57:40` very first time
<br> `57:42` now we go to the threshold value to this
<br> `57:44` bifurcation point
<br> `57:46` and the idea is that we
<br> `57:49` linearize around that suppose now you
<br> `57:52` have a system yeah so
<br> `57:54` think back about uh our original our
<br> `57:57` lecture from last time there we had
<br> `57:59` rotational
<br> `58:00` invariance now so we have rotation and
<br> `58:03` variance so we're pointing in different
<br> `58:05` directions
<br> `58:06` and then we ask how can we break
<br> `58:08` rotational
<br> `58:09` variance how can we make the system
<br> `58:13` globally point into one direction
<br> `58:16` and now we ask a similar question so we
<br> `58:18` start with a system that is
<br> `58:21` translationally invariant so that it's
<br> `58:24` homogeneous in space so we move it
<br> `58:26` around

### slide 13

<br> `58:27` now from here to there and it doesn't
<br> `58:29` change and that means it's a homogeneous
<br> `58:32` in space now there's no structure in it
<br> `58:36` now how can we now break translational
<br> `58:39` invariance that's a similar question to
<br> `58:42` what we had about the
<br> `58:44` rotational invariance so how
<br> `58:47` and under which condition is a
<br> `58:49` translational invariant is broken
<br> `58:50` and the idea is that we start
<br> `58:54` with a homogeneous solution yeah
<br> `58:58` let's go back here we start with a
<br> `59:00` solution
<br> `59:01` where we have no pattern i like this
<br> `59:04` branch here
<br> `59:05` and the y-axis is something like that
<br> `59:07` quantifies a pattern
<br> `59:09` now so we have here we have this
<br> `59:10` homogeneous state
<br> `59:12` and then we look at small perturbations
<br> `59:14` around that and if we say
<br> `59:16` we hope that if we understand small
<br> `59:19` perturbations around this homogeneous
<br> `59:21` state
<br> `59:22` then we can actually learn something
<br> `59:24` about the real
<br> `59:25` macroscopic states that evolve
<br> `59:29` and that works
<br> `59:33` this idea works if we have something
<br> `59:36` like here
<br> `59:37` you know if we have something like here
<br> `59:39` this picture
<br> `59:40` where a pattern continuously emerge
<br> `59:43` emerges so we exchange some control
<br> `59:46` parameter
<br> `59:48` and then if we change this for control
<br> `59:50` parameter we first get a very weak
<br> `59:52` pattern
<br> `59:53` we get a stronger pattern and even
<br> `59:55` stronger pattern and so on
<br> `59:57` so this this bifurcation of how we get a
<br> `60:00` pattern is continuous
<br> `60:01` and one example here is the
<br> `60:03` supercritical
<br> `60:05` pitchfork bifurcation that is depicted
<br> `60:07` here
<br> `60:08` or that is resembled in a homogeneous
<br> `60:11` system by this kind of bifurcation
<br> `60:13` so how does this work also what we do is
<br> `60:16` we say
<br> `60:17` that our state that has a spatial
<br> `60:20` dependence and a time dependence
<br> `60:23` if gear is given by some homogeneous
<br> `60:25` state now we say the system is stable
<br> `60:28` in some boring homogeneous state
<br> `60:32` and then we have a little perturbation
<br> `60:34` around it
<br> `60:36` and now we ask whether this perturbation
<br> `60:38` will grow
<br> `60:39` or not and we're not asking just about
<br> `60:43` any percolation we make a specific
<br> `60:47` answer for these perturbations you know
<br> `60:51` to make it answers
<br> `60:55` oh sorry wrong color
<br> `61:00` bring ons us
<br> `61:03` for the growth
<br> `61:08` of periodic perturbations
<br> `61:15` let me see if i have that oh i don't
<br> `61:18` have two answers already here
<br> `61:19` okay so okay great so here we see
<br> `61:22` i don't have to write that down so we
<br> `61:25` make it answers for
<br> `61:26` periodic perturbations yeah and this
<br> `61:29` answers
<br> `61:31` looks as following that we say that our
<br> `61:33` little perturbation here
<br> `61:35` that we with a linear order because our
<br> `61:38` little perturbation has two components
<br> `61:41` one component describes
<br> `61:45` the time evolution of our perturbation
<br> `61:49` now and depending that has some rate
<br> `61:51` here some pre-factor sigma q
<br> `61:53` and whether the sigma q is positive or
<br> `61:56` negative
<br> `61:57` tells us whether this perturbation will
<br> `61:59` grow or shrink
<br> `62:02` and then we ask here then we have here
<br> `62:04` this
<br> `62:05` imaginary part now this can also be an
<br> `62:07` imaginary
<br> `62:08` this is this this complex part
<br> `62:11` where we have essentially a periodic
<br> `62:14` pattern
<br> `62:15` now that's the complex representation of
<br> `62:17` a periodic
<br> `62:18` pattern and here we have a pattern that
<br> `62:21` has
<br> `62:22` a wave vector q
<br> `62:26` and now we ask if we make this answer
<br> `62:28` for some
<br> `62:29` values of q
<br> `62:32` now for some values of q we have this
<br> `62:34` periodic perturbation
<br> `62:36` around this homogeneous state does it
<br> `62:38` grow
<br> `62:39` or does it not grow and we ask this
<br> `62:41` question
<br> `62:42` for every value of this wavelength
<br> `62:46` with which we perturb the homogeneous
<br> `62:48` states

### slide 14

<br> `62:51` yeah and then several things can happen
<br> `62:54` that's another
<br> `62:55` transition uh now several uh things can
<br> `62:58` happen
<br> `62:58` so if the real part of the sigma that is
<br> `63:02` a function of q
<br> `63:03` in the end is negative
<br> `63:06` yeah then this homogeneous state is
<br> `63:10` stable
<br> `63:10` we say it's linearly stable and
<br> `63:13` and because this homogeneous state is
<br> `63:15` stable we don't expect to see any
<br> `63:17` spatial structure
<br> `63:19` to emerge now is this phi
<br> `63:24` if this real part of
<br> `63:27` sigma q is positive yeah
<br> `63:31` then this term here grows and grows and
<br> `63:33` grows
<br> `63:35` now then if for some value of q this is
<br> `63:38` positive
<br> `63:39` then we get a pattern because our
<br> `63:42` periodic activation
<br> `63:43` grows and constantly becomes bigger and
<br> `63:45` bigger
<br> `63:47` now if you look at this here we can have
<br> `63:49` this suppose we get this
<br> `63:51` sigma of q we get the rate of growth for
<br> `63:54` each vector for each wave vector q
<br> `63:56` here then we can plot this as a function
<br> `64:00` of our control parameter
<br> `64:03` and what you sometimes see is that this
<br> `64:06` function
<br> `64:07` has some function and it's always
<br> `64:09` negative
<br> `64:10` here and then at some value of r
<br> `64:14` of this control parameter we start
<br> `64:17` intersecting
<br> `64:18` with this zero point
<br> `64:22` and one wave vector
<br> `64:25` begins growing while the others are
<br> `64:27` still suppressed
<br> `64:28` and then if you increase r further
<br> `64:32` then uh you have a broader
<br> `64:35` number of a broader range of wave
<br> `64:38` vectors
<br> `64:39` that that start growing
<br> `64:42` and this wave vector qc of this
<br> `64:45` wavelength the corresponding wavelength
<br> `64:49` of our perturbation that for the first
<br> `64:51` time
<br> `64:53` becomes positive yeah in this
<br> `64:55` bifurcation
<br> `64:56` when we start seeing a pattern this we
<br> `64:59` say
<br> `65:00` gives us the wavelength of the final
<br> `65:02` pattern that gives us the length
<br> `65:04` of the final pattern and of course there
<br> `65:08` for this to work so we need to be very
<br> `65:10` optimistic are we
<br> `65:12` linear a lot we linearize you know we
<br> `65:15` say okay so this is something like this
<br> `65:17` and then we make this answer
<br> `65:22` we make an answers that uh and then
<br> `65:25` we say okay so this unlocks although
<br> `65:27` it's very small it describes whatever is
<br> `65:29` happening
<br> `65:30` on very large scales on on
<br> `65:33` even if we waited for a very long time
<br> `65:36` no
<br> `65:36` and this works very often but a
<br> `65:38` situation where it doesn't work
<br> `65:40` as you can see from here is where this
<br> `65:43` bifurcation is actually not continuous
<br> `65:45` but discrete
<br> `65:46` now for example like in super critical
<br> `65:49` in subcritical
<br> `65:50` bifurcations where you suddenly jump to
<br> `65:52` a pattern forming state
<br> `65:53` then this linear stability or
<br> `65:55` instability analysis
<br> `65:56` does not work
<br> `66:00` so yeah yes
<br> `66:03` is it in the chat at all okay
<br> `66:09` let's see how we can see the chat here
<br> `66:18` ah no you know for some reason
<br> `66:22` for some reason i can't see the chat can
<br> `66:25` you tell me the questions
<br> `66:34` awesome so what kind of perturbation you
<br> `66:36` also said you hope
<br> `66:37` i i saw i hope so i have most cancelling
<br> `66:40` headphones so i hope i have uh
<br> `66:44` the question correctly so there's a
<br> `66:46` question of what kind of perturbation do
<br> `66:47` you put into this
<br> `66:48` state so that so you hope that it
<br> `66:52` doesn't matter
<br> `66:53` now but the simplest answers you can
<br> `66:55` make
<br> `66:57` is just what i've shown here yeah could
<br> `67:00` have shown you
<br> `67:01` of course you can make different
<br> `67:02` perturbations now that are more
<br> `67:04` complicated but then the mathematics
<br> `67:06` gets too complicated and of course
<br> `67:08` what's happened here is i'll show you
<br> `67:10` now a
<br> `67:10` full calculation of this i'll show you
<br> `67:12` an example of course what happens here
<br> `67:14` is
<br> `67:15` what you could do is you just go to what
<br> `67:18` you're doing here is to go to various
<br> `67:19` space
<br> `67:20` yeah this perturbation that wrote down
<br> `67:22` and down here is something like the
<br> `67:23` fourier transform
<br> `67:25` of your perturbation yeah and then you
<br> `67:28` say that
<br> `67:28` one wave vector is the important one so
<br> `67:31` that these wave vectors don't really mix
<br> `67:34` so that's that's the idea behind that
<br> `67:36` now but the
<br> `67:37` idea is in linear stability and
<br> `67:38` stability analysis and that's why
<br> `67:40` they're
<br> `67:41` you always have to check it with other
<br> `67:43` methods uh
<br> `67:44` is that uh of course the kind of
<br> `67:48` perturbation if the kind of
<br> `67:49` perturbations that you make
<br> `67:51` here would be important for the end
<br> `67:53` result then this whole thing wouldn't
<br> `67:55` work
<br> `67:56` and it only works of course because you
<br> `67:58` are allowed to linearize
<br> `68:00` and uh because you assume that these
<br> `68:02` different
<br> `68:03` q values don't interact with each other
<br> `68:07` in some some some complex way
<br> `68:10` i don't know if this was the question is
<br> `68:12` basically you put in some
<br> `68:13` some very weak uh periodic perturbation
<br> `68:18` you know so you can have this unzots
<br> `68:21` which is essentially like a sine or
<br> `68:22` cosine
<br> `68:24` and see if this answer grows
<br> `68:27` or shrinks and that then tells you
<br> `68:31` how your what in the linear regime the
<br> `68:34` linear approximation
<br> `68:36` uh how your system reacts to
<br> `68:38` perturbations
<br> `68:40` and then you assume that if you wait
<br> `68:43` long enough you look macroscopically at
<br> `68:45` your pattern
<br> `68:46` like a jupiter that the wavelength that
<br> `68:49` grows strongest
<br> `68:52` once you go through this bifurcation
<br> `68:53` here this wavelength that grows
<br> `68:55` strongest is the one that will actually
<br> `68:58` then dominate
<br> `68:59` also in the long term
<br> `69:02` yeah so that's what you think it works
<br> `69:04` it works very well yeah so
<br> `69:06` but only under constraints under certain
<br> `69:08` conditions
<br> `69:11` um before i show you a specific example
<br> `69:14` there of course is there's a whole
<br> `69:16` classification
<br> `69:17` of these instabilities of how you can
<br> `69:20` generate a pattern
<br> `69:21` and that also depends it all depends on
<br> `69:25` how our sigma of q this function sigma
<br> `69:28` of q
<br> `69:31` looks like as you increase this r
<br> `69:34` parameter
<br> `69:35` that drives us from the homogeneous
<br> `69:37` state to a pattern state
<br> `69:39` now for example if you have there's a
<br> `69:41` type one instability that i just showed
<br> `69:43` you
<br> `69:44` and uh this is so in this type 1
<br> `69:47` instability
<br> `69:48` you have this parabola like shaped where
<br> `69:51` you have a maximum
<br> `69:53` at a finite wavelength or wave factor to
<br> `69:56` finite wave vector
<br> `69:57` and there's one specific wave vector
<br> `70:01` that will start growing uh
<br> `70:06` in a very well defined way now so so
<br> `70:08` here you have one specific
<br> `70:10` finite wave vector that will
<br> `70:14` dominate this process and that's called
<br> `70:16` type 1 and stability
<br> `70:18` there's a type 2 instability as well and
<br> `70:20` that's a little bit complicated so let's
<br> `70:22` let's
<br> `70:23` let's maybe first start with the type
<br> `70:24` well this is a type 3 instability
<br> `70:27` and there also you have a wave vector
<br> `70:30` that has the dominant
<br> `70:32` growth well that has the
<br> `70:35` the maximum of this function sigma of q
<br> `70:39` uh in this case is at q equals zero
<br> `70:44` also wave vector zero and wave x zero
<br> `70:48` means that you have a very long
<br> `70:49` wavelength
<br> `70:50` and that means your whole system is
<br> `70:52` essentially homogeneous
<br> `70:53` so instabilities of types type three
<br> `70:59` gives you situations where actually we
<br> `71:01` go from homogeneous state
<br> `71:03` to another homogeneous state the reason
<br> `71:06` why these
<br> `71:06` instabilities are important is that you
<br> `71:09` can also have situations
<br> `71:11` where the sigma of q has uh
<br> `71:14` an imaginary part and if the sigma of q
<br> `71:18` has an
<br> `71:18` imaginary part then this first part here
<br> `71:22` also
<br> `71:22` describes an oscillation yeah then you
<br> `71:25` have an oscillation not only in space
<br> `71:27` but also in time so you can have also
<br> `71:31` instabilities where you actually go from
<br> `71:33` a homogeneous state
<br> `71:35` to an oscillating state that can have a
<br> `71:37` pattern now that can have
<br> `71:38` a wavelength or it can be homogeneous
<br> `71:40` but it can be oscillating
<br> `71:43` and that's one of the prime examples of
<br> `71:44` the type three instabilities
<br> `71:46` now the type two instability in the
<br> `71:48` middle is a little bit
<br> `71:49` uh subtle because here you have
<br> `71:52` the the value factor q so the
<br> `71:56` homogeneous state
<br> `71:57` is always marginally marginal uh
<br> `72:00` marginally unstable the others has this
<br> `72:03` sigma of q
<br> `72:05` uh of of zero so it doesn't really know
<br> `72:08` whether to grow or shrink
<br> `72:10` and then as you increase your control
<br> `72:12` parameter
<br> `72:13` another wavelength becomes important
<br> `72:16` and ultimately dominates the system
<br> `72:20` if your value of r is large enough so
<br> `72:23` here you can have both so it's not
<br> `72:24` really clear what you get
<br> `72:25` you can have a uniform pattern or you
<br> `72:28` can have something that is just a very
<br> `72:30` large pattern with a very large long
<br> `72:32` wavelength
<br> `72:34` so and what you see here these three
<br> `72:36` kinds of qualitative instabilities that
<br> `72:38` you get
<br> `72:39` of how you can get from a homogeneous
<br> `72:41` state
<br> `72:42` to a pattern state that is described by
<br> `72:45` the wavelength or by a wavelet away
<br> `72:47` vector
<br> `72:48` resembles some kind of universality
<br> `72:52` and why did we get here in rosalita why
<br> `72:54` is there why are there only these three
<br> `72:56` types where
<br> `72:57` can we understand a large class of
<br> `73:00` dynamical systems
<br> `73:02` by just three classes the reason is that
<br> `73:05` we restricted ourselves to situations
<br> `73:07` that look like this here where we
<br> `73:10` linearize
<br> `73:12` where we can linearize around this
<br> `73:15` homogeneous state
<br> `73:16` and then suddenly when we linearize all
<br> `73:19` other complexities
<br> `73:21` become unimportant yeah
<br> `73:25` so this type of instability that you get
<br> `73:27` tells you a lot about what kind of
<br> `73:29` pattern
<br> `73:30` you have

### slide 15

<br> `73:33` now let's have a look at a simple
<br> `73:35` example
<br> `73:37` yeah so i already mentioned briefly the
<br> `73:40` swift hohenberg equation
<br> `73:42` now that's this one here
<br> `73:46` we have the second order derivatives and
<br> `73:48` the fourth order
<br> `73:50` derivatives and then we have a linear
<br> `73:53` term
<br> `73:54` in phi and a non-linear sorry
<br> `73:59` that's fine
<br> `74:03` and the normally determined file and now
<br> `74:06` we make this ansas
<br> `74:08` that phi is equal to the homogeneous
<br> `74:10` state
<br> `74:12` plus some perturbation around this
<br> `74:16` and uh what we now do
<br> `74:21` what we now do is we linear-wise we say
<br> `74:24` that
<br> `74:25` we just look at very small perturbations
<br> `74:27` around this how much in the state
<br> `74:29` and we make our answers
<br> `74:37` we make our own dots at delta phi
<br> `74:41` is equal to some constant a either we
<br> `74:44` don't know
<br> `74:45` e to the power of sigma q times t
<br> `74:49` e to the i q x
<br> `74:53` and now we substitute this unless
<br> `75:00` into this swift homework equation
<br> `75:09` and what we get is now a relation
<br> `75:12` between sigma q and q
<br> `75:16` ah so what we get is what is called
<br> `75:19` dispersion relation we got a relation
<br> `75:22` sigma q
<br> `75:25` r minus q squared minus 1
<br> `75:28` squared now that's our dispersion
<br> `75:32` relation
<br> `75:33` and what you have in this dispersion
<br> `75:36` relation here
<br> `75:38` you can see on the left hand side that's
<br> `75:40` that's what you get if you plot it
<br> `75:42` now so for small values of r you have
<br> `75:45` this blue shape
<br> `75:46` and as you increase r you get
<br> `75:50` larger this this function moves up
<br> `75:53` that's just a constitute moves up
<br> `75:56` and then at some point you pierce
<br> `75:57` through this point
<br> `76:00` at a certain value of qc
<br> `76:04` yeah so
<br> `76:07` at this value of qc
<br> `76:11` dominant wavelength
<br> `76:14` or wave vector
<br> `76:22` is just equal to 1.
<br> `76:26` now we ask what is the growth rate
<br> `76:31` at the maximum so the growth rate
<br> `76:39` at the maximum at the maximum
<br> `76:43` of the real part of qc
<br> `76:47` well and this is just of cube or sigma
<br> `76:50` sorry
<br> `76:50` sigma q
<br> `76:54` just plug that n we get r
<br> `77:00` yeah so this maximum moves linearly up
<br> `77:03` and of course you could have already
<br> `77:04` guessed that just from the
<br> `77:05` shape of this here
<br> `77:09` yeah so what this means is that we get
<br> `77:14` pattern formation
<br> `77:19` for r
<br> `77:23` larger than zero no for r larger than
<br> `77:27` zero
<br> `77:27` we have a dominant wavelength we have a
<br> `77:29` perturbation
<br> `77:31` and this perturbation in this linear
<br> `77:34` approximation
<br> `77:35` grows if r
<br> `77:39` is larger than zero yeah then we have a
<br> `77:42` periodic perturbation and also that of
<br> `77:45` some wavelength
<br> `77:46` we put a different kinds of parotid
<br> `77:48` deviations of perturbations with
<br> `77:50` different wavelengths like short
<br> `77:51` wavelength
<br> `77:52` long rate law wavelength and then we see
<br> `77:55` which
<br> `77:56` of these perturbations survives and
<br> `77:58` which has the fastest growth rate
<br> `78:00` and that's what we say gives us the
<br> `78:02` pattern on the long
<br> `78:03` time and on the large scale now so here

### slide 16

<br> `78:07` is a computer simulation of this
<br> `78:09` equation now
<br> `78:11` equation and this is the kind of pattern
<br> `78:14` that you get
<br> `78:15` and get to see in these equations and
<br> `78:18` of course i didn't tell you anything
<br> `78:21` about
<br> `78:22` how this pattern looks like what i the
<br> `78:25` only thing
<br> `78:26` that this linear stability analysis
<br> `78:27` gives you
<br> `78:29` is that you get the wavelength now so
<br> `78:32` you got here
<br> `78:33` the uh typical length scale
<br> `78:36` of such a pattern and you get the
<br> `78:39` conditions
<br> `78:40` under which such a pattern can emerge
<br> `78:43` now and in our case as the toneberg
<br> `78:45` equation we get these kind of patterns
<br> `78:48` once r is larger than zero
<br> `78:51` and of course the systems are more
<br> `78:52` complicated there's always a dynamic
<br> `78:54` belief that's only for example you have
<br> `78:56` to make sure that you understand
<br> `78:58` what's going on at the boundaries you
<br> `79:01` know so these boundaries can be very
<br> `79:03` very
<br> `79:03` important in selecting between
<br> `79:06` different kinds of patterns
<br> `79:11` okay so to conclude let me see okay
<br> `79:14` to conclude let me just have a look at
<br> `79:16` the time

### slide 17

<br> `79:20` oh okay so we're we're talking quite a
<br> `79:22` while
<br> `79:23` okay so to conclude just give let's give
<br> `79:26` me
<br> `79:26` let me give you another example here
<br> `79:28` without going to mathematical details
<br> `79:30` but it's a very important example and
<br> `79:32` this example is called a reaction
<br> `79:34` diffusion equation
<br> `79:36` and it's called the reaction diffusion
<br> `79:37` equation because it consists
<br> `79:39` describes systems that consist
<br> `79:42` of reactions and diffusion now so for
<br> `79:46` example
<br> `79:46` here the time evolution of our field
<br> `79:49` phi of x t is described
<br> `79:52` by some local function or some local
<br> `79:55` reactions
<br> `79:56` they are for example susceptible to
<br> `79:59` infected
<br> `79:59` also some local reactions
<br> `80:03` plus a diffusion term plus diffusion so
<br> `80:05` random motion
<br> `80:07` and so so if i told you that the
<br> `80:09` susceptible
<br> `80:10` this successful and affected people
<br> `80:13` were running around randomly in space
<br> `80:16` then
<br> `80:17` this dynamics would be described by a
<br> `80:20` direction
<br> `80:21` diffusion equation so these are the two
<br> `80:23` components of a reaction diffusion
<br> `80:24` equation
<br> `80:26` and these equations have a famous
<br> `80:29` result that is named after alan turing
<br> `80:33` and what he showed is that you need an
<br> `80:34` erection diffusion system
<br> `80:36` you can get patterns so that at this
<br> `80:38` point people
<br> `80:40` were did not believe that you have
<br> `80:42` diffusion
<br> `80:43` now you have diffusion something that
<br> `80:45` smooths down everything
<br> `80:47` and you can get a pattern and alan
<br> `80:51` turing
<br> `80:52` studied the conditions under which you
<br> `80:54` can get patterns
<br> `80:55` in such reaction diffusion systems and
<br> `80:58` what he basically said is that you need
<br> `80:59` at least two components there will be
<br> `81:01` two chemicals
<br> `81:03` and then he wrote down equations of this
<br> `81:07` form
<br> `81:09` and then he did exactly what we did now
<br> `81:12` for this general equation what we did in
<br> `81:13` the previous
<br> `81:14` minutes namely it conducted a linear
<br> `81:18` instability analysis so you linearize
<br> `81:21` these equations and if you linearize a
<br> `81:23` general equation
<br> `81:24` you get here derivatives or some
<br> `81:27` jacobians
<br> `81:28` now of these functions and you get the
<br> `81:31` conditions that relate
<br> `81:33` the jacobian so the the linear behavior
<br> `81:36` of these functions
<br> `81:37` with the diffusion constant yeah and
<br> `81:39` what he then said
<br> `81:41` is okay if you want to have a pattern in
<br> `81:44` the erection diffusion
<br> `81:45` system with two components then one
<br> `81:48` species needs to be an activator
<br> `81:51` so it needs to be positively regulating
<br> `81:53` itself
<br> `81:54` and the other species needs to be an
<br> `81:57` inhibitor so it's negatively regulating
<br> `81:59` itself
<br> `82:00` and the other activator
<br> `82:03` and the second condition is that this
<br> `82:06` activator diffuses very
<br> `82:08` fuses very slowly and this inhibitor
<br> `82:11` diffuses fastly
<br> `82:15` so how can you get a pattern with that i
<br> `82:17` don't go through the calculation here
<br> `82:19` but the way you get a pattern here is
<br> `82:21` that you have a homogeneous
<br> `82:24` system and you have a little
<br> `82:26` perturbation
<br> `82:27` on the wavelength like we did in the
<br> `82:28` mathematical enzymes
<br> `82:30` then this activator activates itself
<br> `82:34` yeah it will grow but it
<br> `82:37` this activation but it will not smear
<br> `82:39` out you know the diffusion of this
<br> `82:41` activator
<br> `82:42` here is low while the inhibitor
<br> `82:46` also gets activated but it diffuses away
<br> `82:50` so locally the activator can build up
<br> `82:53` the concentration peak while the
<br> `82:55` inhibitor
<br> `82:57` spreads out and that's how you get a
<br> `82:59` pattern
<br> `83:00` in such a touring system and the
<br> `83:04` applicability of such turing systems is
<br> `83:06` of course
<br> `83:07` limited by this conditions here
<br> `83:10` now you need to have a diffusion cons
<br> `83:12` you have two components you need to have
<br> `83:13` a diffusion
<br> `83:14` difference of the diffusion constant of
<br> `83:17` a factor of 10 or so or 40
<br> `83:20` to see these effects and this is very
<br> `83:22` difficult
<br> `83:23` to achieve in biological systems
<br> `83:26` yeah one example where this seems to be
<br> `83:29` implemented
<br> `83:30` is lymph development i think that's
<br> `83:31` chicken here
<br> `83:33` where you see i think that's at the wing
<br> `83:34` of a chicken
<br> `83:36` how this evolves in
<br> `83:39` early development you know or in
<br> `83:41` development
<br> `83:42` of a chicken on the embryo and you can
<br> `83:45` see
<br> `83:46` here that you have these red regions
<br> `83:49` are regions where certain genes are
<br> `83:51` expressed now that are important for
<br> `83:53` development of bones or something like
<br> `83:55` this
<br> `83:56` and you can see how here this pattern
<br> `84:00` this touring pattern is established but
<br> `84:03` again like in the previous cases you
<br> `84:05` have a specific wavelength
<br> `84:08` yeah you see you know that gives you
<br> `84:12` a specific size of your body parts
<br> `84:15` of these fingers and
<br> `84:19` how does it work here despite having
<br> `84:23` this strong assumption on this
<br> `84:26` difference and diffusion constant
<br> `84:28` so this difference in diffusion
<br> `84:29` constants you only need if you have
<br> `84:31` really two components now if you have
<br> `84:33` four or ten components
<br> `84:35` then of course you can get an
<br> `84:37` instability
<br> `84:38` that gives rise to a pattern in a
<br> `84:41` touring system
<br> `84:42` even for um much weaker differences
<br> `84:46` in these diffusion content and in
<br> `84:47` biologically relevant contexts
<br> `84:51` okay so with this at uh i'd like to
<br> `84:53` finish so next week we'll start
<br> `84:56` digging more into epidemics or using
<br> `84:58` epidemics as an excuse
<br> `85:01` to do some non-equilibrium physics and
<br> `85:04` i'll hang around a little bit if your
<br> `85:06` case is there
<br> `85:07` there are any questions otherwise see
<br> `85:09` you next week bye
`85:28
<br> [Music` 
<br> `85:36` oh there's a lot of things going on in
<br> `85:37` the chat
<br> `85:40` um 